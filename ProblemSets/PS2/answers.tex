% Created 2019-01-22 Tue 22:34
% Intended LaTeX compiler: pdflatex
\documentclass[12pt, letterpaper]{paper}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{grffile}
\usepackage{longtable}
\usepackage{wrapfig}
\usepackage{rotating}
\usepackage[normalem]{ulem}
\usepackage{amsmath}
\usepackage{textcomp}
\usepackage{amssymb}
\usepackage{capt-of}
\usepackage{hyperref}
\usepackage{Schwieg}
\usepackage{natbib}
\usepackage{tikz}
\usepackage{bm}
\usepackage{minted}
\usepackage[margin=1in]{geometry}
\usepackage{fontspec}
\setmonofont{DejaVu Sans Mono}[Scale=MatchLowercase]
\author{Timothy Schwieg}
\date{\today}
\title{Structural Estimation Pset 2}
\hypersetup{
 pdfauthor={Timothy Schwieg},
 pdftitle={Structural Estimation Pset 2},
 pdfkeywords={},
 pdfsubject={},
 pdfcreator={Emacs 26.1 (Org mode 9.1.9)}, 
 pdflang={English}}
\begin{document}

\maketitle

\section{Question One}
\label{sec:orgdb13621}
\subsection{a}
\label{sec:org3ff56b5}
\begin{minted}[frame=lines,fontsize=\scriptsize,xleftmargin=\parindent,linenos,mathescape,breaklines=true,stripnl=true,firstnumber=last]{julia}
healthClaims = DataFrame(load("clms.csv", header_exists=false, colnames=["A"]))

results = [["mean", "min", "median", "max", "StdDev"] cln.([mean(healthClaims[:A]), minimum(healthClaims[:A]), median(healthClaims[:A]), maximum(healthClaims[:A]), std(healthClaims[:A])] )]
\end{minted}

\begin{center}
\begin{tabular}{lr}
mean & 720.28\\
min & 0.01\\
median & 172.21\\
max & 2.2797 \texttimes{} 10\(^{\text{05}}\)\\
StdDev & 3972.9\\
\end{tabular}
\end{center}

\begin{minted}[frame=lines,fontsize=\scriptsize,xleftmargin=\parindent,linenos,mathescape,breaklines=true,stripnl=true,firstnumber=last]{julia}
histogram( healthClaims[:A], bins=1000, normalize = true, label="Health Claims")
savefig("histOne.png")

\end{minted}

\begin{center}
\includegraphics[width=.9\linewidth]{histOne.png}
\end{center}

\begin{minted}[frame=lines,fontsize=\scriptsize,xleftmargin=\parindent,linenos,mathescape,breaklines=true,stripnl=true,firstnumber=last]{julia}
  #We force all bins to have length 8, and allow for 100 of them.
histogram( healthClaims[:A], bins=0:8:800, normalize=true, xlims=(0,800),label="Health Claims \$\\leq 800\$")
savefig("histTwo.png")

\end{minted}

\begin{center}
\includegraphics[width=.9\linewidth]{histTwo.png}
\end{center}

We can see the shape of the distribution for majority of the data
points lie below \(800\). There is a very large tail that distorts the
histogram, preventing anything from being seen on the first one. All
we are able to see is that there is a large amount of mass somewhere
slightly above zero in the first one. The second distribution shows
the mode, and indicates the very long tail that the distribution is
likely to contain.

\subsection{b}
\label{sec:org4493d65}
\begin{minted}[frame=lines,fontsize=\scriptsize,xleftmargin=\parindent,linenos,mathescape,breaklines=true,stripnl=true,firstnumber=last]{julia}
function GammaLogLikelihood( x::Vector{Float64}, α::Float64, β::Float64)
    #Yes I know I could get this using Distributions.jl which could
    #even do the MLE estimate But thats pretty much cheating, and
    #gamma is in the exponential family so using Newton's method will
    #cause no issues.

    #Pdf is: $\frac{ 1 }{\Gamma(\alpha) \beta^{\alpha}} x^{\alpha - 1} \exp\left( - \frac{x}{\beta} \right)$
    #Log-likelihood is: $- \alpha \log ( \beta) - \log( \Gamma (\alpha)) + (\alpha - 1) \log x - \frac{x}{\beta}$

    return -α*log( β) - lgamma(α) + (α - 1)*mean(log.(x)) - mean(x) / β
end

function GammaGradient( x::Vector{Float64}, α::Float64, β::Float64)
    delA = -log(β) - digamma(α) + mean(log.(x))
    #delB = mean(x) / β - α
    delB = mean(x) / β^2 - α / β
    return [delA,delB]
end

function GammaHessian( x::Vector{Float64}, α::Float64, β::Float64)
    delAA = -trigamma(α)
    delAB = -1 / β
    delBB =( α / (β*β)) - ((2* mean(x)) / (β*β*β))
    return [delAA delAB; delAB delBB]
end

function GammaPDF( α::Float64, β::Float64, x::Float64)
    return  (1 / (gamma(α)*β^α))*x^(α-1)*exp( -x/β)
end

function EstimateGammaParameters( data::Vector{Float64}, guess::Vector{Float64}, gradientFun, hessianFun)

    θ = guess
    tol = 1e-10
    maxLoops = 100

    grad = gradientFun( data, θ... )
    hess = hessianFun( data, θ... )

    loopCounter = 0
    while( loopCounter < maxLoops && norm(grad) >= tol)
        θ = θ - hess \ grad
        grad = gradientFun( data, θ... )
        hess = hessianFun( data, θ... )

        loopCounter += 1
        # println( norm(grad))
        # println( θ)
        # println( " ")
    end
    # println( loopCounter)
    return θ
end
healthCosts = convert(  Vector{Float64}, healthClaims[:A] )

β₀ =  var(healthCosts) / mean(healthCosts)
α₀ = mean(healthCosts) / β₀

(Gamma_̂α, Gamma_̂β) = EstimateGammaParameters( healthCosts, [α₀, β₀], GammaGradient, GammaHessian)

likelihood = GammaLogLikelihood(  healthCosts, Gamma_̂α, Gamma_̂β)

result = [["\$\\est{\\alpha}\$: ", "\$\\est{\\beta}\$: ", "Likelihood: " ] cln.([ Gamma_̂α,  Gamma_̂β, likelihood])]
\end{minted}

\begin{center}
\begin{tabular}{lr}
\(\est{\alpha}\): & 0.47251\\
\(\est{\beta}\): & 1524.4\\
Likelihood: & -7.3193\\
\end{tabular}
\end{center}

\begin{minted}[frame=lines,fontsize=\scriptsize,xleftmargin=\parindent,linenos,mathescape,breaklines=true,stripnl=true,firstnumber=last]{julia}
histogram( healthClaims[:A], bins=0:8:800, normalize=true, xlims=(0,800),label="Health Claims \$\\leq 800\$")
pdfXVal = range( 0.0, 800.0)

pdfYVal = [GammaPDF( Gamma_̂α, Gamma_̂β, x ) for x in pdfXVal]

plot!( pdfXVal, pdfYVal, label="Gamma Estimate" )
savefig("histPDF_Gamma.png")
\end{minted}

\begin{center}
\includegraphics[width=.9\linewidth]{histPDF_Gamma.png}
\end{center}

We can see that this fit over-fits the tail of the distribution at the
cost of the bulk of the mass. It places a relatively high probability
of being at a very small value, when the distribution appears to have
a hump.
\section{c}
\label{sec:org7fb1ca9}
\begin{minted}[frame=lines,fontsize=\scriptsize,xleftmargin=\parindent,linenos,mathescape,breaklines=true,stripnl=true,firstnumber=last]{julia}
# $\text{(GG):}\quad f(x;\alpha,\beta,m) = \frac{m}{\beta^\alpha \Gamma\left(\frac{\alpha}{m}\right)}x^{\alpha-1}e^{-\left(\frac{x}{\beta}\right)^m},\quad x\in[0,\infty), \:\alpha,\beta,m>0$
function GGammaPDF( α::Float64, β::Float64, m::Float64, x::Float64)
    return ( (m / β^α) * x^(α-1) * exp( - (x / β)^m) ) / gamma( α / m)
end


function GGammaLikelihood( x::Vector{Float64}, α::Real, β::Real, m::Real)
    return log(m) - α*log(β) + (α - 1)*mean(log.(x)) - mean( (x ./ β).^m  ) - lgamma( α / m )    
end

function EstimateGG( data::Vector{Float64}, guess::Vector{Float64})
    #To hard enforce that all of our parameters are positive, we
    #exponentiate them. Limit them to .1 as the lower bound for
    #numerics sake
    θ = log.(guess .- .1)
    fun(x::Vector) = -GGammaLikelihood( data, (exp.(x).+ .1)... )



    result = optimize(fun, θ, Newton(), autodiff=:forward)
end


sln = EstimateGG( healthCosts, [Gamma_̂α, Gamma_̂β, 1.0])

GG_̂α = exp(sln.minimizer[1]) + .1
GG_̂β = exp(sln.minimizer[2]) + .1
GG_̂m = exp(sln.minimizer[3]) + .1
GG_LogLikelihood = -sln.minimum

println( "GG ̂α = ", GG_̂α)
println( "GG ̂β = ", GG_̂β )
println( "GG ̂m = ", GG_̂m )
println( "Likelihood Value: ", GG_LogLikelihood )

result = [["GG \$\\est{\\alpha}\$: ", "GG \$\\est{\\beta}\$: ", "GG \$\\est{m}\$: ","GG Likelihood: " ] cln.([ GG_̂α,  GG_̂β,  GG_̂m, GG_LogLikelihood])]

\end{minted}

\begin{center}
\begin{tabular}{lr}
GG \(\est{\alpha}\): & 1.7396\\
GG \(\est{\beta}\): & 0.1\\
GG \(\est{m}\): & 0.24872\\
GG Likelihood: & -7.0746\\
\end{tabular}
\end{center}

\begin{minted}[frame=lines,fontsize=\scriptsize,xleftmargin=\parindent,linenos,mathescape,breaklines=true,stripnl=true,firstnumber=last]{julia}
histogram( healthClaims[:A], bins=0:8:800, normalize=true, xlims=(0,800),label="Health Claims \$\\leq 800\$")
pdfXVal = range(0.0, 800.0)
#pdfXVal = linspace( minimum(truncatedHealthClaims), maximum(truncatedHealthClaims))
pdfYVal = [GGammaPDF( GG_̂α, GG_̂β, GG_̂m, x ) for x in pdfXVal]

plot!( pdfXVal, pdfYVal, label="Generalized Gamma Estimate" )
savefig( "histPDF_GG.png" )
\end{minted}

\begin{center}
\includegraphics[width=.9\linewidth]{histPDF_GG.png}
\end{center}

This distribution captures the mode of the distribution being greater
than zero, and while the hump is still occurring too early in order to
fit the long tail of the distribution; it appears to fit the histogram
much better than the Gamma Distribution fit.

\subsection{d}
\label{sec:orgf20253f}
\begin{minted}[frame=lines,fontsize=\scriptsize,xleftmargin=\parindent,linenos,mathescape,breaklines=true,stripnl=true,firstnumber=last]{julia}
function GBetaTwoPDF( x::Float64, a::Real, b::Real, p::Real, q::Real)
    #We require all parameters to be positive, so abs(a) = a
    return a*x^(a*p -1) / (b^(a*p) *beta(p,q)*(1+(x/b)^a)^(p+q))
end

function GBetaTwoLikelihood( x::Vector{Float64}, a::Real, b::Real, p::Real, q::Real)
    return log( a) + (a*p -1)*mean(log.(x)) - (a*p)*log(b) - log(beta(p,q)) - (p+q)*mean( log.( 1 .+(x ./ b).^a ))
end

function EstimateGBetaTwo( data::Vector{Float64}, guess::Vector{Float64})
      #To hard enforce that all of our parameters are positive, we
      #exponentiate them
    θ = log.(guess .- .1)
    #θ = guess
    fun(x::Vector) = -GBetaTwoLikelihood( data, (exp.(x) .+ .1)... )


    #This guy is being fickle, Newton() struggles a little bit, but
    #NewtonTrust seems to outperform LBFGS
    result = optimize(fun, θ, NewtonTrustRegion(), autodiff=:forward, Optim.Options(iterations=2000) )
end

#$GG(\alpha,\beta,m) = \lim_{q\rightarrow\infty}GB2\left(a=m,b=q^{1/m}\beta,p=\frac{\alpha}{m},q\right)$
sln = EstimateGBetaTwo( healthCosts, [GG_̂m, 10000^(1 / GG_̂m) * GG_̂β, GG_̂α / GG_̂m, 10000])

GB2_̂α = exp( sln.minimizer[1]) + .1
GB2_̂β = exp( sln.minimizer[2]) + .1
GB2_̂p = exp( sln.minimizer[3]) + .1
GB2_̂q = exp( sln.minimizer[4]) + .1
GB2_LogLikelihood = -sln.minimum

result = [["GB2 \$\\est{\\alpha}\$: ", "GB2 \$\\est{\\beta}\$: ", "GB2 \$\\est{p}\$: ","GB2 \$\\est{q}\$: ","GB2 Likelihood: " ] cln.([GB2_̂α, GB2_̂β,  GB2_̂p,  GB2_̂q, -sln.minimum])]
\end{minted}

\begin{center}
\begin{tabular}{lr}
GB2 \(\est{\alpha}\): & 1.2714\\
GB2 \(\est{\beta}\): & 143.23\\
GB2 \(\est{p}\): & 1.0299\\
GB2 \(\est{q}\): & 0.84852\\
GB2 Likelihood: & -7.0354\\
\end{tabular}
\end{center}

\begin{minted}[frame=lines,fontsize=\scriptsize,xleftmargin=\parindent,linenos,mathescape,breaklines=true,stripnl=true,firstnumber=last]{julia}
histogram( healthClaims[:A], bins=0:8:800, normalize=true, xlims=(0,800),label="Health Claims \$\\leq 800\$")
pdfXVal = range( 0.0, 800.0)
#pdfXVal = linspace( minimum(truncatedHealthClaims), maximum(truncatedHealthClaims))
pdfYVal = [GBetaTwoPDF( x, GB2_̂α, GB2_̂β, GB2_̂p, GB2_̂q ) for x in pdfXVal]

plot!( pdfXVal, pdfYVal, label="Generalized Beta 2 Estimate" )
savefig( "histPDF_GB2.png" )
\end{minted}

\begin{center}
\includegraphics[width=.9\linewidth]{histPDF_GB2.png}
\end{center}

We can see that the Generalized Beta 2 Distribution has fit the
distribution near \(0\) slightly better than the Generalized Gamma
Distribution did. It still captures the long tail of the distribution
relatively well, though the fit is only slightly better than the
previous one. 

\subsection{e}
\label{sec:org65bd701}
Since the likelihood function values at the optimum for parts (b) and
(c) are the constrained maximum likelihood estimators, the likelihood
ratio test is simply: 
\begin{equation*}
  2 \left( f( \est{\theta}) - f(\altest{\theta}) \right) \sim \chi_{p}^{2}
\end{equation*}

Where \(p\) is the number of constraints in the estimation procedure. 
\begin{minted}[frame=lines,fontsize=\scriptsize,xleftmargin=\parindent,linenos,mathescape,breaklines=true,stripnl=true,firstnumber=last]{julia}
# Gamma Has Two restrictions
tStatGamma = 2*N*(GB2_LogLikelihood - likelihood)
# Generalized Gamma Has One Restriction
tStatGG = 2*N*(GB2_LogLikelihood - GG_LogLikelihood)

results = [["", "Gamma", "Generalized Gamma"] [ "\$\\chi^{2}\$", cln(tStatGamma), cln(tStatGG)] ["p-value",  cln(1.0 - cdf(Chisq(4),tStatGamma)), cln(1.0 - cdf( Chisq(4),tStatGG)) ] ]
\end{minted}

\begin{center}
\begin{tabular}{lrl}
 & \(\chi^{2}\) & p-value\\
Gamma & 56.771 & 1.382 \texttimes{} 10\(^{\text{-11}}\)\\
Generalized Gamma & 7.8294 & 0.098033\\
\end{tabular}
\end{center}

We find that we can reject the Null Hypothesis that the parameters of
the Generalized Beta 2 are consistent with the Gamma Distribution at
pretty much any significance level. We find that the probability that
this data could be generated by a Gamma Distribution is virtually
zero.

For the Generalized Gamma Distribution, we find that it is possible
that these parameters are consistent with the Generalized Gamma
Distribution. To be willing to reject this hypothesis, we must be
willing to accept a 10\% chance of being incorrect. Since we are not
psychologists, we will fail to reject this hypothesis.

\subsection{f}
\label{sec:orgcb22e3b}
The Probability that someone has a health care claim of more than
\$$\backslash$\(1000\) is given by:

\begin{align*}
  \Pr( X > 1000) &= 1 - \Pr( X \leq 1000)\\
                 &= \int_0^{1000}f_Xdx
\end{align*}

However, since the integral of a Generalized Beta 2 Distribution is
quite nasty, I shall compute it numerically. We ignore more
complicated methods of quadrature and brute force rhomboid
quadrature. 

\begin{minted}[frame=lines,fontsize=\scriptsize,xleftmargin=\parindent,linenos,mathescape,breaklines=true,stripnl=true,firstnumber=last]{julia}
f(x) = GBetaTwoPDF( x, GB2_̂α, GB2_̂β, GB2_̂p, GB2_̂q )
area = quadgk( f, 0, 1000 )[1]
output = ["Probability of Having > 1000: " cln(1-area)]
\end{minted}

\begin{center}
\begin{tabular}{lr}
Probability of Having > 1000: & 0.11766\\
\end{tabular}
\end{center}

We would like to do the same for the Gamma Distribution as well. 

\begin{minted}[frame=lines,fontsize=\scriptsize,xleftmargin=\parindent,linenos,mathescape,breaklines=true,stripnl=true,firstnumber=last]{julia}
f(x) = GammaPDF( Gamma_̂α, Gamma_̂β, x )
area = quadgk(f, 0, 1000)[1]
output = ["Gamma Probability of Having > 1000: " cln(1-area)]
\end{minted}

\begin{center}
\begin{tabular}{lr}
Gamma Probability of Having > 1000: & 0.23678\\
\end{tabular}
\end{center}

We can see that the Gamma Distribution overstates the long tail of the
distribution, as it is difficult for this distribution to fit a large
amount of data very far away from the mean.

\section{Question 2}
\label{sec:orge15ef94}

\subsection{a}
\label{sec:org54fc3fd}

Equations (3) and (5) tell us that


\begin{align*}
  w_t - (1-\alpha) exp( z_t ) (k_t)^{\alpha} &= 0\\
  z_t = \rho z_{t-1} + (1-\rho)\mu &+ \epsilon_t
\end{align*}

Taking logs of equation (3):
\begin{align*}
  \log w_t &= \log ( 1- \alpha) + z_t + \alpha \log k_t\\
  z_t &= \log w_t - \log ( 1- \alpha) - \alpha \log k_t
\end{align*}

This tells us that for $t > 1$
\begin{align*}
  \log w_t - \log ( 1- \alpha) - \alpha \log k_t &\sim \normal\left( \rho z_{t-1} +
                                             (1-\rho)\mu, \sigma^2 \right)\\
  &\sim \normal\left( \rho\left( \log w_{t-1} - \log( 1- \alpha) - \alpha \log
    k_{t-1} \right) + (1-\rho)\mu, \sigma^2 \right)
\end{align*}

For $t=1$
\begin{equation*}
  \log w_1 - \log ( 1- \alpha) - \alpha \log k_1 \sim \normal( \mu, \sigma^2)
\end{equation*}


We may now estimate this model using Maximum Likelihood Estimation

\begin{minted}[frame=lines,fontsize=\scriptsize,xleftmargin=\parindent,linenos,mathescape,breaklines=true,stripnl=true,firstnumber=last]{julia}
#$\normal\left( \rho\left( \log w_{t-1} - \log( 1- \alpha) -(\alpha-1) \log k_{t-1} \right) + (1-\rho)\mu, \sigma^2 \right)$

#Clean it up when it exists, comes in the order: (c, k, w, r)
macroData = DataFrame(load("MacroSeries.csv", header_exists=false, colnames=["C", "K", "W", "R"]))

w = convert( Vector{Float64}, macroData[:W] )
k = convert( Vector{Float64}, macroData[:K] )

function LogLikelihood( N, w::Vector{Float64}, k::Vector{Float64}, α::Real, ρ::Real, μ::Real, σ²::Real  )
    #The pdf of a normal: $\frac{1}{\sqrt{2 \pi \sigma^2}} \exp( - \frac{ (x-\mu)^2}{2 \sigma^2})$
    #Log Likelihood: $- \frac{1}{2} \log \sigma^2 - \frac{ (x-\mu)^2}{ 2 \sigma^2}$

    logLik = -.5*log(σ²)- (( log(w[1]) - log(1-α) - (α)*log(k[1]) - μ)^2 / (2*σ²))

    #Note we do not have the -.5*log(2*pi)
    #Because that does not matter at all for MLE estimation.
    for i in 2:N
        mean = ρ*(log(w[i-1]) - log( 1 - α)  - (α)*log( k[i-1])) + (1-ρ)*μ
        logLik += -.5*log( σ² ) - (  (log(w[i]) - log(1-α) - (α)*log(k[i]) - mean)^2 / (2*σ²))
    end
    return logLik
end


N = length(w)

α₀ = .5
β = .99
μ₀ = .5
σ₀ = .5
ρ₀ = 0.0

#We parameterize each of the variables so that they meet their constraints.
# tanh is used to ensure that $\rho \in (-1,1)$
θ = zeros(4)
θ[1] = log( α₀ / ( 1 - α₀) )
θ[2] = atanh( ρ₀)
θ[3] = log( μ₀ )
θ[4] = log( σ₀)


fun(x::Vector) = -LogLikelihood( N, w, k, exp(x[1]) / (1 + exp(x[1])), tanh(x[2]), exp(x[3]), exp(x[4])  )

result = optimize(fun, θ, Newton(), autodiff=:forward)

model_̂θ = result.minimizer

model_̂α = exp(model_̂θ[1]) / (1 + exp(model_̂θ[1]))
model_̂ρ = tanh(model_̂θ[2])
model_̂μ = exp(model_̂θ[3])
model_̂σ = exp(model_̂θ[4])

output = [["\$\\est{\\alpha}\$:", "\$\\est{\\rho}\$:", "\$\\est{\\mu}\$:", "\$\\est{\\sigma^{2}}\$:"]  cln.([model_̂α, model_̂ρ, model_̂μ, model_̂σ])]
\end{minted}

\begin{center}
\begin{tabular}{lr}
\(\est{\alpha}\): & 0.70216\\
\(\est{\rho}\): & 0.47972\\
\(\est{\mu}\): & 6.2533\\
\(\est{\sigma^{2}}\): & 0.0084723\\
\end{tabular}
\end{center}

\begin{minted}[frame=lines,fontsize=\scriptsize,xleftmargin=\parindent,linenos,mathescape,breaklines=true,stripnl=true,firstnumber=last]{julia}

#Sadly Optim.jl does not automatically report the hessian, though I am
#sure it is obtainable. So we will use forward-mode automatic
#differentiation to obtain this hessian. However it does not always
#return symmetric matrices, so we will make the matrix symmetric then
#invert it using the cholesky decomposition to be numerically stable.
hess = ForwardDiff.hessian(fun, result.minimizer)

F = cholesky(Hermitian(hess))
F.L * F.U = H
hessInv = cln.(F.U \ (F.L \ I))
#This is for version .6 rather than the 1.0 running above.
#F = chol(Hermitian(hess))
#hessInv = cln.(F \ (F' \ I))
result = hessInv
\end{minted}

\begin{equation*}
  \inv{H} =
  \begin{pmatrix}
    1.2234 & -0.38792 & -0.50942 & -2.1141 \times 10^{-12}\\
-0.38792 & 0.1361 & 0.16153 & 2.6498 \times 10^{-12}\\
-0.50942 & 0.16153 & 0.21213 & 3.384 \times 10^{-13}\\
-2.1141 \times 10^{-12} & 2.6498 \times 10^{-12} & 3.384 \times 10^{-13} & 0.02
  \end{pmatrix}
\end{equation*}

% \begin{center}
% \begin{tabular}{rrrl}
% 1.2234 & -0.38792 & -0.50942 & -2.1141 \texttimes{} 10\(^{\text{-12}}\)\\
% -0.38792 & 0.1361 & 0.16153 & 2.6498 \texttimes{} 10\(^{\text{-12}}\)\\
% -0.50942 & 0.16153 & 0.21213 & 3.384 \texttimes{} 10\(^{\text{-13}}\)\\
% -2.1141 \texttimes{} 10\(^{\text{-12}}\) & 2.6498 \texttimes{} 10\(^{\text{-12}}\) & 3.384 \texttimes{} 10\(^{\text{-13}}\) & 0.02\\
% \end{tabular}
% \end{center}


We can see that the model believes that there is almost no co-variance
between the \(\sigma^2\) and the other parameters. There is a high standard
error for \(\alpha\) and \(\sigma^2\) relative to the magnitude of the point
estimate. 

\section{b}
\label{sec:orgfb4996d}

Equations (4) and (5) read:
\begin{align*}
  r_t - \alpha \exp( z_t ) k_t^{\alpha -1 } &= 0\\
  z_t = \rho z_{t-1} + (1-\rho)\mu &+ \epsilon_t\\
  \epsilon_t \sim \normal( 0, \sigma^2)
\end{align*}

Taking logs and isolating $z_t$
\begin{align*}
  \log r_t  &= \log \alpha + (\alpha-1) \log k_t + z_t\\
  z_t &=  \log r_t - \log \alpha - (\alpha-1) \log k_t
\end{align*}

For $t > 1$:
\begin{align*}
  \log r_t - \log \alpha - (\alpha-1) \log k_t &\sim \normal\left( \rho z_{t-1} +
                                       (1-\rho)\mu, \sigma^2 \right)\\
  &\sim \normal\left( \rho\left( \log r_{t-1} - \log \alpha - (\alpha-1)\log k_{t-1}
    \right) + (1-\rho)\mu, \sigma^2 \right)
\end{align*}

For $t = 1$:
\begin{equation*}
  \log r_1 - \log \alpha - (\alpha-1)\log k_1 \sim \normal( \mu, \sigma^2)
\end{equation*}

This can be estimated using an MLE.

\begin{minted}[frame=lines,fontsize=\scriptsize,xleftmargin=\parindent,linenos,mathescape,breaklines=true,stripnl=true,firstnumber=last]{julia}
r = convert( Vector{Float64}, macroData[:R] )
k = convert( Vector{Float64}, macroData[:K] )

#$\log r_t - \log \alpha - z_t - (\alpha - 1 ) \log k_t = 0$
function LogLikelihood( N, r::Vector{Float64}, k::Vector{Float64}, α::Real, ρ::Real, μ::Real, σ²::Real  )
    #The pdf of a normal: $\frac{1}{\sqrt{2 \pi \sigma^2}} \exp( - \frac{ (x-\mu)^2}{2 \sigma^2})$
    #Log Likelihood: $- \frac{1}{2} \log \sigma^2 - \frac{ (x-\mu)^2}{ 2 \sigma^2}$

    logLik = -.5*log(σ²)- (( log(r[1]) - log(α) - (α-1)*log(k[1]) - μ)^2 / (2*σ²))

    #Note we do not have the -.5*log(2*pi)
    #Because that does not matter at all for MLE estimation.
    for i in 2:N
        mean = ρ*(log(r[i-1]) - log( α)  - (α-1)*log( k[i-1])) + (1-ρ)*μ
        logLik += -.5*log( σ² ) - (  (log(r[i]) - log(α) - (α-1)*log(k[i]) - mean)^2 / (2*σ²))
    end

    return logLik
end

N = size(macroData)[1]

α₀ = .5
β = .99
μ₀ = .5
σ₀ = .5
ρ₀ = 0.0

#We parameterize each of the variables so that they meet their
# constraints.  tanh is used to ensure that $\rho \in (-1,1)$
θ = zeros(4)
θ[1] = log( α₀ / ( 1 - α₀) )
θ[2] = atanh( ρ₀)
θ[3] = log( μ₀ )
θ[4] = log( σ₀)

function limitedLogistic( unbounded::Real )
    return ((exp(unbounded)) / ( 1 + exp(unbounded)))*.99 + .005
end

#This clamp on the logistic function is quite the hack, since this
#function shouldn't get to 0 or 1, but it was getting stuck at 1
fun(x::Vector) = -LogLikelihood( N, r, k, limitedLogistic(x[1]), tanh(x[2]), exp(x[3]), exp(x[4])  )

result = optimize(fun, θ, Newton(), autodiff=:forward)

bmodel_̂θ = result.minimizer

bmodel_̂α = limitedLogistic(bmodel_̂θ[1])
bmodel_̂ρ = tanh(bmodel_̂θ[2])
bmodel_̂μ = exp(bmodel_̂θ[3])
bmodel_̂σ = exp(bmodel_̂θ[4])

output = [["\$\\est{\\alpha}\$:", "\$\\est{\\rho}\$:", "\$\\est{\\mu}\$:", "\$\\est{\\sigma^{2}}\$:"]  cln.([bmodel_̂α, bmodel_̂ρ, bmodel_̂μ, bmodel_̂σ])]
\end{minted}

\begin{center}
\begin{tabular}{lr}
\(\est{\alpha}\): & 0.70216\\
\(\est{\rho}\): & 0.47972\\
\(\est{\mu}\): & 5.0729\\
\(\est{\sigma^{2}}\): & 0.0084723\\
\end{tabular}
\end{center}

\begin{minted}[frame=lines,fontsize=\scriptsize,xleftmargin=\parindent,linenos,mathescape,breaklines=true,stripnl=true,firstnumber=last]{julia}
#Sadly Optim.jl does not automatically report the hessian, though I am
#sure it is obtainable. So we will use forward-mode automatic
#differentiation to obtain this hessian. However it does not always
#return symmetric matrices, so we will make the matrix symmetric then
#invert it using the cholesky decomposition to be numerically stable.
hess = ForwardDiff.hessian(fun, result.minimizer)

F = cholesky(Hermitian(hess))
#F.U' * F.U = H
hessInv = cln.(F.U \ (F.L \ I))
# F = chol(Hermitian(hess))
# hessInv = cln.(F \ (F' \ I))
result = hessInv
\end{minted}

% \begin{center}
%   \begin{tabular}{rrrl}

\begin{equation*}
  \inv{H} =
  \begin{pmatrix}
    1.2582 & -0.3934 & -0.88139 & -3.7224 \times 10^{-13}\\
-0.3934 & 0.1361 & 0.27559 & 1.1806 \times 10^{-13}\\
-0.88139 & 0.27559 & 0.61745 & 2.6018 \times 10^{-13}\\
-3.7224 \times 10^{-13} & 1.1806 \times 10^{-13} & 2.6018 \times 10^{-13} & 0.02
  \end{pmatrix}
\end{equation*}

% \end{tabular}
% \end{center}

We find nearly the same results for the point estimates, and the
diagonal elements of the inverse Hessian, modulo some noise. We find
that the off-diagonal elements are less consistent between the two
estimates, though these co-variances are quite small relative to the
measurements. To really tell the difference between the point
estimates, we would have to compare the overlap of the confidence
sets. 

\subsection{c}
\label{sec:org83a11f9}
From the derivation of the distribution of $\log r_t$ in part (b):

\begin{align*}
    \Pr( r_t > 1) &= \Pr( \log r_t > 0)\\
                  &= \Pr( \log \alpha + z_t + (\alpha - 1)\log k_t > 0)\\
                  &= \Pr( \log \alpha + \rho z_{t-1} + (1 - \rho)\mu + \epsilon_t + (\alpha-1) \log k_t > 0)\\
    &= \Pr( \log(\alpha) + \rho z_{t-1} + (1-\rho)\mu + \sigma Z + (\alpha-1) \log k_t
      > 0)\\
                  &= \Pr( Z > - \frac{1}{\sigma} ( \log(\alpha) + \rho z_{t-1} + (1-\rho)\mu + (\alpha-1)\log k_t))\\
    &= 1 - \Pr( Z \leq - \sigma ( \log(\alpha) + \rho z_{t-1} + (1-\rho)\mu + (\alpha-1)\log
      k_t))\\
    &\approx 1 - \Pr( Z \leq  -\frac{1}{\est{\sigma}} ( \log \est{\alpha} + \est{\rho}10 + (1-\est{\rho})
      \est{\mu} + (\est{\alpha} - 1) \log( 7,500,000) ))\\
\end{align*}

Where $Z \sim \normal(0,1)$

\begin{minted}[frame=lines,fontsize=\scriptsize,xleftmargin=\parindent,linenos,mathescape,breaklines=true,stripnl=true,firstnumber=last]{julia}
prob = 1 - cdf( Normal(), -(1.0 / sqrt(model_̂σ))*( log(model_̂α) + model_̂ρ*10 + (1-model_̂ρ)*model_̂μ + (model_̂α-1)*log( 7500000)))
result = ["\\Pr( r_t > 1) = " cln(prob)]
\end{minted}

\begin{center}
\begin{tabular}{lr}
\(\Pr\)( r\(_{\text{t}}\) > 1) = & 1\\
\end{tabular}
\end{center}
\end{document}