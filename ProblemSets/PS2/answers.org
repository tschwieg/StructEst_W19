#+OPTIONS: toc:nil 
#+TITLE: Structural Estimation Pset 2
#+AUTHOR: Timothy Schwieg
#+PROPERTY: header-args :cache yes :exports both :tangle yes
#+PROPERTY: header-args:julia :session *julia*

#+LaTeX_CLASS: paper
#+LaTeX_CLASS_OPTIONS: [12pt, letterpaper]

#+LATEX_HEADER: \usepackage[margin=1in]{geometry}
#+LATEX_HEADER: \usepackage{fontspec}
#+LATEX_HEADER: \setmonofont{DejaVu Sans Mono}[Scale=MatchLowercase]

* Question One
#+BEGIN_SRC julia :exports none
  using Plots
  using DataFrames
  using CSVFiles
  using ForwardDiff
  using Distributions
  using SpecialFunctions
  using Optim
  using LinearAlgebra
  using QuadGK
  using Printf

  function cln( x::Float64 )
      return replace(@sprintf("%.5g",x), r"e[\+]?([\-0123456789]+)" => s" \\times 10^{\1}")  
  end
  pyplot()
#+END_SRC


#+RESULTS[f60b46cf9783c2d60eb7782211c4315879281938]:



** a
#+BEGIN_SRC julia 
  #healthClaims = CSV.read( "clms.txt", header=[:A] )
  healthClaims = DataFrame(load("clms.csv", header_exists=false, colnames=["A"]))
  #describe( healthClaims )
  #println( "Standard Deviation: ", std(healthClaims[:A]))

  results = [["mean", "min", "median", "max", "StdDev"] cln.([mean(healthClaims[:A]), minimum(healthClaims[:A]), median(healthClaims[:A]), maximum(healthClaims[:A]), std(healthClaims[:A])] )]
#+END_SRC

#+RESULTS[12d8ef4db482e1ba6e5cafccdaa66692079353d5]:
| mean   |        720.28 |
| min    |          0.01 |
| median |        172.21 |
| max    | 2.2797 \times 10^{05} |
| StdDev |        3972.9 |

#+BEGIN_SRC julia :results graphics  :file histOne.png
  histogram( healthClaims[:A], bins=1000, normalize = true, label="Health Claims")
  savefig("histOne.png")
#+END_SRC

#+RESULTS[3dd520f05a48a965777c0a514ee944095490b689]:
[[file:histOne.png]]

#+BEGIN_SRC julia :results graphics :file histTwo.png
    #We force all bins to have length 8, and allow for 100 of them.
  histogram( healthClaims[:A], bins=0:8:800, normalize=true, xlims=(0,800),label="Health Claims \$\\leq 800\$")
  savefig("histTwo.png")
#+END_SRC

#+RESULTS[35907771fbff13aa1b781daf33722e9a6f69c7c7]:
[[file:histTwo.png]]



** b
#+BEGIN_SRC julia :results value
  function GammaLogLikelihood( x::Vector{Float64}, α::Float64, β::Float64)
      #Yes I know I could get this using Distributions.jl which could
      #even do the MLE estimate But thats pretty much cheating, and
      #gamma is in the exponential family so using Newton's method will
      #cause no issues.

      #Pdf is: $\frac{ 1 }{\Gamma(\alpha) \beta^{\alpha}} x^{\alpha - 1} \exp\left( - \frac{x}{\beta} \right)$
      #Log-likelihood is: $- \alpha \log ( \beta) - \log( \Gamma (\alpha)) + (\alpha - 1) \log x - \frac{x}{\beta}$

      return -α*log( β) - lgamma(α) + (α - 1)*mean(log.(x)) - mean(x) / β
  end

  function GammaGradient( x::Vector{Float64}, α::Float64, β::Float64)
      delA = -log(β) - digamma(α) + mean(log.(x))
      #delB = mean(x) / β - α
      delB = mean(x) / β^2 - α / β
      return [delA,delB]
  end

  function GammaHessian( x::Vector{Float64}, α::Float64, β::Float64)
      delAA = -trigamma(α)
      delAB = -1 / β
      delBB =( α / (β*β)) - ((2* mean(x)) / (β*β*β))
      return [delAA delAB; delAB delBB]
  end

  function GammaPDF( α::Float64, β::Float64, x::Float64)
      return  (1 / (gamma(α)*β^α))*x^(α-1)*exp( -x/β)
  end

  function EstimateGammaParameters( data::Vector{Float64}, guess::Vector{Float64}, gradientFun, hessianFun)

      θ = guess
      tol = 1e-10
      maxLoops = 100

      grad = gradientFun( data, θ... )
      hess = hessianFun( data, θ... )

      loopCounter = 0
      while( loopCounter < maxLoops && norm(grad) >= tol)
          θ = θ - hess \ grad
          grad = gradientFun( data, θ... )
          hess = hessianFun( data, θ... )

          loopCounter += 1
          # println( norm(grad))
          # println( θ)
          # println( " ")
      end
      #println( loopCounter)
      return θ
  end
  healthCosts = convert(  Vector{Float64}, healthClaims[:A] )

  β₀ =  var(healthCosts) / mean(healthCosts)
  α₀ = mean(healthCosts) / β₀

  (Gamma_̂α, Gamma_̂β) = EstimateGammaParameters( healthCosts, [α₀, β₀], GammaGradient, GammaHessian)

  likelihood = GammaLogLikelihood(  healthCosts, Gamma_̂α, Gamma_̂β)

  result = [["\$\\est{\\alpha}\$: ", "\$\\est{\\beta}\$: ", "Likelihood: " ] cln.([ Gamma_̂α,  Gamma_̂β, likelihood])]

#+END_SRC

#+RESULTS[43c50541d8d4328b59805832a4b9382146fc2319]:
| $\est{\alpha}$:  | 0.47251 |
| $\est{\beta}$:  |  1524.4 |
| Likelihood: | -7.3193 |

#+BEGIN_SRC julia  :results value graphics :file histPDF_Gamma.png
  histogram( healthClaims[:A], bins=0:8:800, normalize=true, xlims=(0,800),label="Health Claims \$\\leq 800\$")
  pdfXVal = range( 0.0, 800.0)
  #pdfXVal = linspace( minimum(truncatedHealthClaims), maximum(truncatedHealthClaims))
  pdfYVal = [GammaPDF( Gamma_̂α, Gamma_̂β, x ) for x in pdfXVal]


  plot!( pdfXVal, pdfYVal, label="Gamma Estimate" )
  savefig("histPDF_Gamma.png")
#+END_SRC

#+RESULTS[45a8afb698dd527cfd5bd2ac0bb663293628f410]:
[[file:histPDF_Gamma.png]]

* c
#+BEGIN_SRC julia

  # $\text{(GG):}\quad f(x;\alpha,\beta,m) = \frac{m}{\beta^\alpha \Gamma\left(\frac{\alpha}{m}\right)}x^{\alpha-1}e^{-\left(\frac{x}{\beta}\right)^m},\quad x\in[0,\infty), \:\alpha,\beta,m>0$
  function GGammaPDF( α::Float64, β::Float64, m::Float64, x::Float64)
      return ( (m / β^α) * x^(α-1) * exp( - (x / β)^m) ) / gamma( α / m)
  end


  function GGammaLikelihood( x::Vector{Float64}, α::Real, β::Real, m::Real)
      return log(m) - α*log(β) + (α - 1)*mean(log.(x)) - mean( (x ./ β).^m  ) - lgamma( α / m )    
  end

  function EstimateGG( data::Vector{Float64}, guess::Vector{Float64})
      #To hard enforce that all of our parameters are positive, we
      #exponentiate them. Limit them to .1 as the lower bound for
      #numerics sake
      θ = log.(guess .- .1)
      fun(x::Vector) = -GGammaLikelihood( data, (exp.(x).+ .1)... )



      result = optimize(fun, θ, Newton(), autodiff=:forward)
  end


  sln = EstimateGG( healthCosts, [Gamma_̂α, Gamma_̂β, 1.0])

  GG_̂α = exp(sln.minimizer[1]) + .1
  GG_̂β = exp(sln.minimizer[2]) + .1
  GG_̂m = exp(sln.minimizer[3]) + .1
  GG_LogLikelihood = -sln.minimum

  println( "GG ̂α = ", GG_̂α)
  println( "GG ̂β = ", GG_̂β )
  println( "GG ̂m = ", GG_̂m )
  println( "Likelihood Value: ", GG_LogLikelihood )

  result = [["GG \$\\est{\\alpha}\$: ", "GG \$\\est{\\beta}\$: ", "GG \$\\est{m}\$: ","GG Likelihood: " ] cln.([ GG_̂α,  GG_̂β,  GG_̂m, GG_LogLikelihood])]

#+END_SRC

#+RESULTS[68d572a70054f768f35536d78cb8299e9536101e]:
| GG $\est{\alpha}$:  |  1.7396 |
| GG $\est{\beta}$:  |     0.1 |
| GG $\est{m}$:  | 0.24872 |
| GG Likelihood: | -7.0746 |

#+BEGIN_SRC julia  :results value graphics :file histPDF_GG.png
  histogram( healthClaims[:A], bins=0:8:800, normalize=true, xlims=(0,800),label="Health Claims \$\\leq 800\$")
  pdfXVal = range(0.0, 800.0)
  #pdfXVal = linspace( minimum(truncatedHealthClaims), maximum(truncatedHealthClaims))
  pdfYVal = [GGammaPDF( GG_̂α, GG_̂β, GG_̂m, x ) for x in pdfXVal]


  plot!( pdfXVal, pdfYVal, label="Generalized Gamma Estimate" )
  savefig( "histPDF_GG.png" )
#+END_SRC

#+RESULTS[969c034363739c887914bee4c3e23d419049d91f]:
[[file:histPDF_GG.png]]


** d 
#+BEGIN_SRC julia
  function GBetaTwoPDF( x::Float64, a::Real, b::Real, p::Real, q::Real)
      #We require all parameters to be positive, so abs(a) = a
      return a*x^(a*p -1) / (b^(a*p) *beta(p,q)*(1+(x/b)^a)^(p+q))
  end



  function GBetaTwoLikelihood( x::Vector{Float64}, a::Real, b::Real, p::Real, q::Real)
      return log( a) + (a*p -1)*mean(log.(x)) - (a*p)*log(b) - log(beta(p,q)) - (p+q)*mean( log.( 1 .+(x ./ b).^a ))
  end

  function EstimateGBetaTwo( data::Vector{Float64}, guess::Vector{Float64})
        #To hard enforce that all of our parameters are positive, we
        #exponentiate them
      θ = log.(guess .- .1)
      #θ = guess
      fun(x::Vector) = -GBetaTwoLikelihood( data, (exp.(x) .+ .1)... )


      #This guy is being fickle, Newton() struggles a little bit, but
      #NewtonTrust seems to outperform LBFGS
      result = optimize(fun, θ, NewtonTrustRegion(), autodiff=:forward, Optim.Options(iterations=2000) )
  end

  #$GG(\alpha,\beta,m) = \lim_{q\rightarrow\infty}GB2\left(a=m,b=q^{1/m}\beta,p=\frac{\alpha}{m},q\right)$
  sln = EstimateGBetaTwo( healthCosts, [GG_̂m, 10000^(1 / GG_̂m) * GG_̂β, GG_̂α / GG_̂m, 10000])

  GB2_̂α = exp( sln.minimizer[1]) + .1
  GB2_̂β = exp( sln.minimizer[2]) + .1
  GB2_̂p = exp( sln.minimizer[3]) + .1
  GB2_̂q = exp( sln.minimizer[4]) + .1
  GB2_LogLikelihood = -sln.minimum

  result = [["GB2 \$\\est{\\alpha}\$: ", "GB2 \$\\est{\\beta}\$: ", "GB2 \$\\est{p}\$: ","GB2 \$\\est{q}\$: ","GB2 Likelihood: " ] cln.([GB2_̂α, GB2_̂β,  GB2_̂p,  GB2_̂q, -sln.minimum])]
#+END_SRC

#+RESULTS[44eb59590167354a526d56902f97c55b67e8a88a]:
| GB2 $\est{\alpha}$:  |  1.2714 |
| GB2 $\est{\beta}$:  |  143.23 |
| GB2 $\est{p}$:  |  1.0299 |
| GB2 $\est{q}$:  | 0.84852 |
| GB2 Likelihood: | -7.0354 |

#+BEGIN_SRC julia  :results graphics :file histPDF_GB2.png
  histogram( healthClaims[:A], bins=0:8:800, normalize=true, xlims=(0,800),label="Health Claims \$\\leq 800\$")
  pdfXVal = range( 0.0, 800.0)
  #pdfXVal = linspace( minimum(truncatedHealthClaims), maximum(truncatedHealthClaims))
  pdfYVal = [GBetaTwoPDF( x, GB2_̂α, GB2_̂β, GB2_̂p, GB2_̂q ) for x in pdfXVal]


  plot!( pdfXVal, pdfYVal, label="Generalized Beta 2 Estimate" )
  savefig( "histPDF_GB2.png" )
#+END_SRC

#+RESULTS[52ff72ae16d713cd7bf9ffee3b7a4e9ae5fbcae5]:
[[file:histPDF_GB2.png]]

** e
Since the likelihood function values at the optimum for parts (b) and
(c) are the constrained maximum likelihood estimators, the likelihood
ratio test is simply: 
#+BEGIN_EXPORT latex
  \begin{equation*}
    2 \left( f( \est{\theta} - \altest{\theta}) \right) \sim \chi_{p}^{2}
  \end{equation*}
#+END_EXPORT

Where $p$ is the number of constraints in the estimation procedure. 
#+BEGIN_SRC julia

  # Gamma Has Two restrictions
  tStatGamma = 2*(GB2_LogLikelihood - likelihood)
  # Generalized Gamma Has One Restriction
  tStatGG = 2*(GB2_LogLikelihood - GG_LogLikelihood)

  results = [["", "Gamma", "Generalized Gamma"] [ "\$\\chi^{2}\$", cln(tStatGamma), cln(tStatGG)] ["p-value",  cln(1.0 - cdf(Chisq(4),tStatGamma)), cln(1.0 - cdf( Chisq(4),tStatGG)) ] ]
#+END_SRC

#+RESULTS[4de0ebb3e97eca034d27dbd6c93c794dcc7fd10e]:
|                   |     $\chi^{2}$ | p-value |
| Gamma             |  0.56771 | 0.96658 |
| Generalized Gamma | 0.078294 | 0.99925 |

** f
The Probability that someone has a health care claim of more than
$\$1000$ is given by:

#+BEGIN_EXPORT latex
  \begin{align*}
    \Pr( X > 1000) &= 1 - \Pr( X \leq 1000)\\
                   &= \int_0^{1000}f_Xdx
  \end{align*}
#+END_EXPORT

However, since the integral of a Generalized Beta 2 Distribution is
quite nasty, I shall compute it numerically.

#+BEGIN_SRC julia
  f(x) = GBetaTwoPDF( x, GB2_̂α, GB2_̂β, GB2_̂p, GB2_̂q )
  area = quadgk( f, 0, 1000 )[1]
  output = ["Probability of Having > 1000: " cln(1-area)]
#+END_SRC

#+RESULTS[864af5f419ffa876e7615f9835856d30b55e3953]:
| Probability of Having > 1000: | 0.11766 |



* Question 2

** a

Equations (3) and (5) tell us that


#+BEGIN_EXPORT latex
\begin{align*}
  w_t - (1-\alpha) exp( z_t ) (k_t)^{\alpha-1} &= 0\\
  z_t = \rho z_{t-1} + (1-\rho)\mu &+ \epsilon_t
\end{align*}

Taking logs of equation (3):
\begin{align*}
  \log w_t &= \log ( 1- \alpha) + z_t + (\alpha-1) \log k_t\\
  z_t &= \log w_t - \log ( 1- \alpha) - (\alpha-1) \log k_t
\end{align*}

This tells us that for $t > 1$
\begin{align*}
  \log w_t - \log ( 1- \alpha) - (\alpha-1) \log k_t &\sim \normal\left( \rho z_{t-1} +
                                             (1-\rho)\mu, \sigma^2 \right)\\
  &\sim \normal\left( \rho\left( \log w_{t-1} - \log( 1- \alpha) -(\alpha-1) \log
    k_{t-1} \right) + (1-\rho)\mu, \sigma^2 \right)
\end{align*}

For $t=1$
\begin{equation*}
  \log w_1 - \log ( 1- \alpha) - (\alpha-1) \log k_1 \sim \normal( \mu, \sigma^2)
\end{equation*}


We may now estimate this model using Maximum Likelihood Estimation
  #+END_EXPORT

#+BEGIN_SRC julia
  #$\normal\left( \rho\left( \log w_{t-1} - \log( 1- \alpha) -(\alpha-1) \log k_{t-1} \right) + (1-\rho)\mu, \sigma^2 \right)$

  #Clean it up when it exists, comes in the order: (c, k, w, r)
  macroData = DataFrame(load("MacroSeries.csv", header_exists=false, colnames=["C", "K", "W", "R"]))

  w = convert( Vector{Float64}, macroData[:W] )
  k = convert( Vector{Float64}, macroData[:K] )

  function LogLikelihood( N, w::Vector{Float64}, k::Vector{Float64}, α::Real, ρ::Real, μ::Real, σ²::Real  )
      #The pdf of a normal: $\frac{1}{\sqrt{2 \pi \sigma^2}} \exp( - \frac{ (x-\mu)^2}{2 \sigma^2})$
      #Log Likelihood: $- \frac{1}{2} \log \sigma^2 - \frac{ (x-\mu)^2}{ 2 \sigma^2}$

      logLik = -.5*log(σ²)- ( log(w[1]) - log(1-α) - (1-α)*log(k[1]) - μ)^2 / (2*σ²)

      #Note we do not have the -.5*log(2*pi)
      #Because that does not matter at all for MLE estimation.
      for i in 2:N
          mean = ρ*(log(w[i-1]) - log( 1 - α)  - (α-1)*log( k[i-1])) + (1-ρ)*μ
          logLik += -.5*log( σ² ) - (  (log(w[i]) - log(1-α) - (1-α)*log(k[i]) - mean)^2 / (2*σ²))
      end
      return logLik
  end

  N = length(w)

  α₀ = .5
  β = .99
  μ₀ = 1.0
  σ₀ = 1.0
  ρ₀ = 0.0

  #We parameterize each of the variables so that they meet their constraints.
  # tanh is used to ensure that $\rho \in (-1,1)$
  θ = zeros(4)
  θ[1] = log( α₀ / ( 1 - α₀) )
  θ[2] = atanh( ρ₀)
  θ[3] = log( μ₀ )
  θ[4] = log( σ₀)


  fun(x::Vector) = -LogLikelihood( N, w, k, exp(x[1]) / (1 + exp(x[1])), tanh(x[2]), exp(x[3]), exp(x[4])  )

  result = optimize(fun, θ, Newton(), autodiff=:forward)

  model_̂θ = result.minimizer

  model_̂α = exp(model_̂θ[1]) / (1 + exp(model_̂θ[1]))
  model_̂ρ = tanh(model_̂θ[2])
  model_̂μ = exp(model_̂θ[3])
  model_̂σ = exp(model_̂θ[4])

  output = [["\$\\est{\\alpha}\$:", "\$\\est{\\rho}\$:", "\$\\est{\\mu}\$:", "\$\\est{\\sigma^{2}}\$:"]  cln.([model_̂α, model_̂ρ, model_̂μ, model_̂σ])]
#+END_SRC

#+RESULTS[76349a21deacd9dbb13be58d372a2c4a8a0607b3]:
| $\est{\alpha}$:  |    0.1128 |
| $\est{\rho}$:  | 0.0013758 |
| $\est{\mu}$:  |    2.1987 |
| $\est{\sigma^{2}}$: | 0.0095002 |

#+BEGIN_SRC julia
 #Sadly Optim.jl does not automatically report the hessian, though I am
  #sure it is obtainable. So we will use forward-mode automatic
  #differentiation to obtain this hessian. However it does not always
  #return symmetric matrices, so we will make the matrix symmetric then
  #invert it using the cholesky decomposition to be numerically stable.
  hess = ForwardDiff.hessian(fun, result.minimizer)
  for i in 1:4
      for j in 1:i
          if i == j
              continue
          end
          hess[i,j] = (hess[i,j]+hess[j,i])*.5
          hess[j,i] = hess[i,j]
      end
  end
  F = cholesky(hess)
  #F.L * F.U = H
  hessInv = cln.(F.U \ (F.L \ I))
#+END_SRC

#+RESULTS[33e1303fb3d68519f808575893a665ef1d117a35]:
|         0.22705 | 0.00017156     |         0.17324 | -5.1778 \times 10^{-16} |
|      0.00017156 | 1.2065 \times 10^{-05} |  -2.018 \times 10^{-05} | 4.6392 \times 10^{-18}  |
|         0.17324 | -2.018 \times 10^{-05} |         0.13412 | -4.5417 \times 10^{-16} |
| -5.1778 \times 10^{-16} | 4.6392 \times 10^{-18} | -4.5417 \times 10^{-16} | 0.02            |

* b

#+BEGIN_EXPORT latex
Equations (4) and (5) read:
\begin{align*}
  r_t - \alpha \exp( z_t ) k_t^{\alpha -1 } &= 0\\
  z_t = \rho z_{t-1} + (1-\rho)\mu &+ \epsilon_t\\
  \epsilon_t \sim \normal( 0, \sigma^2)
\end{align*}

Taking logs and isolating $z_t$
\begin{align*}
  \log r_t  &= \log \alpha + (\alpha-1) \log k_t + z_t\\
  z_t &= \log \alpha + (\alpha-1) \log k_t - \log r_t
\end{align*}

For $t > 1$:
\begin{align*}
  \log \alpha + (\alpha-1) \log k_t - \log r_t &\sim \normal\left( \rho z_{t-1} +
                                       (1-\rho)\mu, \sigma^2 \right)\\
  &\sim \normal\left( \rho\left( \log \alpha + (\alpha-1)\log k_{t-1} - \log r_{t-1}
    \right) + (1-\rho)\mu, \sigma^2 \right)
\end{align*}

For $t = 1$:
\begin{equation*}
  \log \alpha + (\alpha-1)\log k_1 - \log r_1 \sim \normal( \mu, \sigma^2)
\end{equation*}

This can be estimated using an MLE.
#+END_EXPORT

#+BEGIN_SRC julia

  r = convert( Vector{Float64}, macroData[:R] )
  k = convert( Vector{Float64}, macroData[:K] )

  #$\log r_t - \log \alpha - z_t - (\alpha - 1 ) \log k_t = 0$

  function LogLikelihood( N, w::Vector{Float64}, k::Vector{Float64}, α::Real, ρ::Real, μ::Real, σ²::Real  )
      #The pdf of a normal: $\frac{1}{\sqrt{2 \pi \sigma^2}} \exp( - \frac{ (x-\mu)^2}{2 \sigma^2})$
      #Log Likelihood: $- \frac{1}{2} \log \sigma^2 - \frac{ (x-\mu)^2}{ 2 \sigma^2}$

      logLik = -.5*log(σ²) - (log(α) + (α-1)*log(k[1]) - log(r[1]) - μ)^2 / (2*σ² )
      #Note the way that the model is structured is: F(...) = 0, so we
      #are maximizing the likelihood of getting a 0 returned for all the
      #moments

      for i in 2:N
          mean = ρ*(log(α) + (α-1)*log(k[i-1]) - log(r[i-1])) + (1-ρ)*μ
          logLik += -.5*log( σ² ) - (  (log(α) + (α-1)*log(k[i]) - log(r[i]) - mean)^2 / (2*σ²))
      end
      return logLik
  end

  N = length(w)

  # α₀ = .5
  # β = .99
  # μ₀ = 1.0
  # σ₀ = 1.0
  # ρ₀ = .99
    α₀ = .5
    β = .99
    μ₀ = 1.0
    σ₀ = 1.0
    ρ₀ = 0.0

  # #We param
  eterize each of the variables so that they meet their constraints.
  # tanh is used to ensure that $\rho \in (-1,1)$
  θ = zeros(4)
  θ[1] = log( α₀ / ( 1 - α₀) )
  θ[2] = atanh( ρ₀)
  θ[3] = log( μ₀ )
  θ[4] = log( σ₀)

  #This clamp on the logistic function is quite the hack, since this
  #function shouldn't get to 0 or 1, but it was getting stuck at 1
  fun(x::Vector) = -LogLikelihood( N, w, k, (exp(x[1]) / (1 + exp(x[1])))*.9+.05, tanh(x[2]), exp(x[3]), exp(x[4])  )

  result = optimize(fun, θ, Newton(), autodiff=:forward)

  model_̂θ = result.minimizer

  model_̂α = (exp(model_̂θ[1]) / (1 + exp(model_̂θ[1])))*.9+.05
  model_̂ρ = tanh(model_̂θ[2])
  model_̂μ = exp(model_̂θ[3])
  model_̂σ = exp(model_̂θ[4])

  output = [["\$\\est{\\alpha}\$:", "\$\\est{\\rho}\$:", "\$\\est{\\mu}\$:", "\$\\est{\\sigma^{2}}\$:"]  cln.([model_̂α, model_̂ρ, model_̂μ, model_̂σ])]
#+END_SRC

#+RESULTS[c07e194a6d2b4f91e4011c1f36aa7257fe64c299]:
| $\est{\alpha}$:  |           0.95 |
| $\est{\rho}$:  |        0.99102 |
| $\est{\mu}$:  | 8.2563 \times 10^{-15} |
| $\est{\sigma^{2}}$: |        0.02061 |

#+BEGIN_SRC julia

  #Sadly Optim.jl does not automatically report the hessian, though I am
   #sure it is obtainable. So we will use forward-mode automatic
   #differentiation to obtain this hessian. However it does not always
   #return symmetric matrices, so we will make the matrix symmetric then
   #invert it using the cholesky decomposition to be numerically stable.
   hess = ForwardDiff.hessian(fun, result.minimizer)
   for i in 1:4
       for j in 1:i
           if i == j
               continue
           end
           hess[i,j] = (hess[i,j]+hess[j,i])*.5
           hess[j,i] = hess[i,j]
       end
   end
   F = cholesky(hess)
   #F.L * F.U = H
   hessInv = cln.(F.U \ (F.L \ I))
#+END_SRC

#+RESULTS[80f308121bac90d77d18d0ef0d4697a0b24a7170]:
| 2.2698 \times 10^{12} |      -0.023973 |     0.0088919 |      -0.014286 |
|     -0.023973 |        0.88359 |      0.031051 | 3.9818 \times 10^{-16} |
|     0.0088919 |       0.031051 | 3.0942 \times 10^{12} |           0.02 |
|     -0.014286 | 3.9818 \times 10^{-16} |          0.02 |           0.02 |

** c
#+BEGIN_EXPORT latex
  From the derivation of the distribution of $\log r_t$ in part (b):

  \begin{align*}
      \Pr( r_t > 1) &= \Pr( \log r_t > 0)\\
                    &= \Pr( \log \alpha + z_t + (\alpha - 1)\log k_t > 0)\\
                    &= \Pr( \log \alpha + \rho z_{t-1} + (1 - \rho)\mu + \epsilon_t + (\alpha-1) \log k_t > 0)\\
      &= \Pr( \log(\alpha) + \rho z_{t-1} + (1-\rho)\mu + \frac{Z}{\sigma} + (\alpha-1) \log k_t
        > 0)\\
                    &= \Pr( Z > - \sigma ( \log(\alpha) + \rho z_{t-1} + (1-\rho)\mu + (\alpha-1)\log k_t))\\
      &= 1 - \Pr( Z \leq - \sigma ( \log(\alpha) + \rho z_{t-1} + (1-\rho)\mu + (\alpha-1)\log
        k_t))\\
                    &= \Phi( - \sigma ( \log(\alpha) + \rho z_{t-1} + (1-\rho)\mu + (\alpha-1)\log k_t ))\\
      &\approx \Phi( -\est{\sigma} ( \log \est{\alpha} + \est{\rho}10 + (1-\est{\rho})
        \est{\mu} + (\est{\alpha} - 1) \log( 7,500,000) ))\\
    \end{align*}
#+END_EXPORT

#+BEGIN_SRC julia
    prob = cdf( Normal(), -sqrt(model_̂σ)*( log(model_̂α) + model_̂ρ*10 + (1-model_̂ρ)*model_̂μ + (model_̂α-1)*log( 7500000)))
  result = ["Prob" cln(prob)]

#+END_SRC

#+RESULTS[8d136a1b79e0febc4a46bd12889b3bffe73c988e]:
| Prob | 0.2541 |
